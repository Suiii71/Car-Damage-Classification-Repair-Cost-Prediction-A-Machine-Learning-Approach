{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "860ab160e1a0429590025727b347b4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6add4f08da74723b822408496628e96",
              "IPY_MODEL_112d9ec0477449fabe62566dd7cc1abe",
              "IPY_MODEL_a5a27392165c444f8136973085b604ec"
            ],
            "layout": "IPY_MODEL_ffd624884c6d4265a9a0958c7e6c80e3"
          }
        },
        "d6add4f08da74723b822408496628e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0f46bf88ac949039c27335919e16b63",
            "placeholder": "​",
            "style": "IPY_MODEL_ede21b36b3944f4595626197cae101cd",
            "value": "model.safetensors: 100%"
          }
        },
        "112d9ec0477449fabe62566dd7cc1abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fee37eb7563747918561d0bf34cb9790",
            "max": 14184144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed4d0b255c444ffea310969f1a74e29a",
            "value": 14184144
          }
        },
        "a5a27392165c444f8136973085b604ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fb3c7b7faa54d48b8760ec34a2543e6",
            "placeholder": "​",
            "style": "IPY_MODEL_facbe96ef0df4da79378caabbe1a1fe8",
            "value": " 14.2M/14.2M [00:00&lt;00:00, 23.7MB/s]"
          }
        },
        "ffd624884c6d4265a9a0958c7e6c80e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0f46bf88ac949039c27335919e16b63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ede21b36b3944f4595626197cae101cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fee37eb7563747918561d0bf34cb9790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed4d0b255c444ffea310969f1a74e29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fb3c7b7faa54d48b8760ec34a2543e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "facbe96ef0df4da79378caabbe1a1fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suiii71/Car-Damage-Classification-Repair-Cost-Prediction-A-Machine-Learning-Approach/blob/main/MobileNet_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQmyXmB_3_01",
        "outputId": "818b2015-eab4-418c-d2a7-1399789ebefa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "5ui5Zo2oz8Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision pandas pillow\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "DngKTv9VztZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b3ba5b-7f71-4312-d475-7858be6223f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = \"/content/drive/MyDrive/Car_Damage_Project\"\n",
        "TRAIN_CSV = os.path.join(ROOT, \"CarDD_classification_train.csv\")\n",
        "TEST_CSV  = os.path.join(ROOT, \"CarDD_classification_test.csv\")\n",
        "\n",
        "TRAIN_IMG_DIR = os.path.join(ROOT, \"train2017\")\n",
        "TEST_IMG_DIR  = os.path.join(ROOT, \"test2017\")"
      ],
      "metadata": {
        "id": "XFAZJnhRFbZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Class"
      ],
      "metadata": {
        "id": "9p5jQDlVz3YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df  = pd.read_csv(TEST_CSV)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(df.head())\n",
        "print(df['label'].value_counts())\n",
        "classes = sorted(train_df[\"label\"].unique())\n",
        "num_classes = len(classes)\n",
        "\n",
        "label_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "idx_to_label = {i:c for c,i in label_to_idx.items()}\n",
        "\n",
        "train_df[\"label_idx\"] = train_df[\"label\"].map(label_to_idx)\n",
        "test_df[\"label_idx\"]  = test_df[\"label\"].map(label_to_idx)\n",
        "\n",
        "classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA4no0CH0LaC",
        "outputId": "b0a0296a-f659-44c6-ef43-8d3614148cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['crack', 'dent', 'glass shatter', 'lamp broken', 'scratch', 'tire flat']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CarDamageDataset(Dataset):\n",
        "    def __init__(self, df, folder, transform=None):\n",
        "        self.df = df\n",
        "        self.folder = folder\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.folder, row[\"image\"])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = row[\"label_idx\"]\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "FOKX-DRYDqn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df2, val_df = train_test_split(\n",
        "    train_df,\n",
        "    test_size=0.15,\n",
        "    stratify=train_df[\"label_idx\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "len(train_df2), len(val_df)"
      ],
      "metadata": {
        "id": "ka7C6T2pzy-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e008ab-c14f-431c-fff1-5f5169bc2ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2393, 423)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "ptpa9k3BFzLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CarDamageDataset(train_df2, TRAIN_IMG_DIR, train_tf)\n",
        "val_dataset   = CarDamageDataset(val_df,   TRAIN_IMG_DIR, test_tf)\n",
        "test_dataset  = CarDamageDataset(test_df,  TEST_IMG_DIR, test_tf)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "d1XWZhNUF49Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.array(range(num_classes)),\n",
        "    y=train_df2[\"label_idx\"].values\n",
        ")\n",
        "\n",
        "weights = torch.tensor(weights, dtype=torch.float32)\n",
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzJC824kF6mL",
        "outputId": "1d7372b9-3db1-4085-f695-30260a3df2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.3236, 0.4852, 1.2013, 6.7599, 0.4373, 2.6948])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
        "model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "LOaUMFIyLSg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(weight=weights.to(device), label_smoothing=0.1)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', patience=2, factor=0.5\n",
        ")\n",
        "\n",
        "EPOCHS = 30\n",
        "best_val = 0\n",
        "patience = 8\n",
        "stop = 0"
      ],
      "metadata": {
        "id": "pTlHIJ0VGWQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out = model(imgs)\n",
        "            _, preds = torch.max(out, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_acc = correct / total\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {running_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val:\n",
        "        best_val = val_acc\n",
        "        stop = 0\n",
        "        torch.save(model.state_dict(), \"mobilenet_best.pth\")\n",
        "    else:\n",
        "        stop += 1\n",
        "\n",
        "    if stop >= patience:\n",
        "        print(\"Early stopping!\")\n",
        "        break\n",
        "\n",
        "print(\"Best Validation Accuracy =\", best_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAS5_gU9GaMp",
        "outputId": "4b9ee521-e2a9-4f14-c8f2-3bb2bce7e807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 | Loss: 160.4101 | Val Acc: 0.1489\n",
            "Epoch 2/30 | Loss: 160.0153 | Val Acc: 0.1489\n",
            "Epoch 3/30 | Loss: 160.8799 | Val Acc: 0.1442\n",
            "Epoch 4/30 | Loss: 161.0709 | Val Acc: 0.1418\n",
            "Epoch 5/30 | Loss: 160.4805 | Val Acc: 0.1442\n",
            "Epoch 6/30 | Loss: 159.5243 | Val Acc: 0.1560\n",
            "Epoch 7/30 | Loss: 161.2978 | Val Acc: 0.1489\n",
            "Epoch 8/30 | Loss: 161.6950 | Val Acc: 0.1489\n",
            "Early stopping!\n",
            "Best Validation Accuracy = 0.38534278959810875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"mobilenet_best.pth\"))\n",
        "model.eval()\n",
        "\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out = model(imgs)\n",
        "        _, preds = torch.max(out, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(\"Test Accuracy =\", correct/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1BRMuQ9GbvU",
        "outputId": "91c0f487-08fb-462c-b2eb-66a4e8cbfe61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy = 0.37967914438502676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "print(train_df.head(20))\n",
        "print(train_df.image.nunique(), len(train_df))\n",
        "\n",
        "print(\"Unique labels:\", train_df.label.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrJBjehESGiS",
        "outputId": "d825c709-1d47-48d3-9ae8-f15c212e5b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         image      label\n",
            "0   000001.jpg  tire flat\n",
            "1   000002.jpg  tire flat\n",
            "2   000003.jpg  tire flat\n",
            "3   000004.jpg  tire flat\n",
            "4   000005.jpg  tire flat\n",
            "5   000006.jpg  tire flat\n",
            "6   000007.jpg  tire flat\n",
            "7   000008.jpg      crack\n",
            "8   000009.jpg  tire flat\n",
            "9   000010.jpg  tire flat\n",
            "10  000011.jpg  tire flat\n",
            "11  000014.jpg    scratch\n",
            "12  000018.jpg    scratch\n",
            "13  000019.jpg    scratch\n",
            "14  000020.jpg    scratch\n",
            "15  000021.jpg    scratch\n",
            "16  000022.jpg    scratch\n",
            "17  000026.jpg    scratch\n",
            "18  000027.jpg    scratch\n",
            "19  000028.jpg       dent\n",
            "2816 2816\n",
            "Unique labels: ['tire flat' 'crack' 'scratch' 'dent' 'glass shatter' 'lamp broken']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8iBTlfrT3p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 0) INSTALLS (Colab)\n",
        "# =========================================================\n",
        "!pip install timm torch torchvision pandas pillow scikit-learn\n",
        "\n",
        "# =========================================================\n",
        "# 1) IMPORTS\n",
        "# =========================================================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# =========================================================\n",
        "# 2) PATHS (CHANGE THESE)\n",
        "# =========================================================\n",
        "ROOT = \"/content/drive/MyDrive/Car_Damage_Project\"\n",
        "TRAIN_CSV = os.path.join(ROOT, \"CarDD_classification_train.csv\")\n",
        "TEST_CSV  = os.path.join(ROOT, \"CarDD_classification_test.csv\")\n",
        "\n",
        "TRAIN_IMG_DIR = os.path.join(ROOT, \"train2017\")\n",
        "TEST_IMG_DIR  = os.path.join(ROOT, \"test2017\")\n",
        "# =========================================================\n",
        "# 3) LOAD + SHUFFLE CSV (CRITICAL FIX)\n",
        "# =========================================================\n",
        "df = pd.read_csv(TRAIN_CSV)\n",
        "\n",
        "# shuffle to remove class blocks (your csv is sorted by label)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(df.head(10))\n",
        "print(\"\\nLabel counts:\\n\", df['label'].value_counts())\n",
        "\n",
        "# =========================================================\n",
        "# 4) FILTER OUT MISSING IMAGES (SAFETY)\n",
        "# =========================================================\n",
        "exists_mask = df[\"image\"].apply(lambda x: os.path.exists(os.path.join(TRAIN_IMG_DIR, x)))\n",
        "missing = df.loc[~exists_mask, \"image\"].tolist()\n",
        "\n",
        "print(\"\\nMissing images:\", len(missing))\n",
        "if len(missing) > 0:\n",
        "    print(\"Example missing:\", missing[:10])\n",
        "\n",
        "df = df[exists_mask].reset_index(drop=True)\n",
        "\n",
        "# =========================================================\n",
        "# 5) STRATIFIED TRAIN/VAL SPLIT (CRITICAL FIX)\n",
        "# =========================================================\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=df[\"label\"]\n",
        ")\n",
        "\n",
        "print(\"\\nTrain split counts:\\n\", train_df[\"label\"].value_counts())\n",
        "print(\"\\nVal split counts:\\n\", val_df[\"label\"].value_counts())\n",
        "\n",
        "# =========================================================\n",
        "# 6) LABEL ENCODING (CONSISTENT ORDER)\n",
        "# =========================================================\n",
        "classes = sorted(df[\"label\"].unique())\n",
        "label_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "idx_to_label = {i:c for c,i in label_to_idx.items()}\n",
        "num_classes = len(classes)\n",
        "\n",
        "train_df[\"label_idx\"] = train_df[\"label\"].map(label_to_idx)\n",
        "val_df[\"label_idx\"]   = val_df[\"label\"].map(label_to_idx)\n",
        "\n",
        "print(\"\\nClasses:\", classes)\n",
        "print(\"num_classes:\", num_classes)\n",
        "\n",
        "# =========================================================\n",
        "# 7) TRANSFORMS (STRONG + NORMALIZATION)\n",
        "# =========================================================\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.3,0.3,0.3,0.1),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.7,1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "])\n",
        "\n",
        "# =========================================================\n",
        "# 8) DATASET CLASS\n",
        "# =========================================================\n",
        "class CarDamageDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row[\"image\"])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = int(row[\"label_idx\"])\n",
        "        return img, label\n",
        "\n",
        "# =========================================================\n",
        "# 9) DATALOADERS\n",
        "# =========================================================\n",
        "train_dataset = CarDamageDataset(train_df, TRAIN_IMG_DIR, train_transform)\n",
        "val_dataset   = CarDamageDataset(val_df, TRAIN_IMG_DIR, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"\\nTrain samples:\", len(train_dataset))\n",
        "print(\"Val samples:\", len(val_dataset))\n",
        "\n",
        "# =========================================================\n",
        "# 10) CLASS WEIGHTS (FIX IMBALANCE)\n",
        "# =========================================================\n",
        "counts = train_df[\"label_idx\"].value_counts().sort_index().values\n",
        "class_weights = 1.0 / counts\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "print(\"\\nClass weights:\", weights)\n",
        "\n",
        "# =========================================================\n",
        "# 11) MODEL: MobileNetV2 (timm)\n",
        "# =========================================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = timm.create_model(\n",
        "    \"mobilenetv2_100\",\n",
        "    pretrained=True,\n",
        "    num_classes=num_classes\n",
        ").to(device)\n",
        "\n",
        "# =========================================================\n",
        "# 12) LOSS, OPTIMIZER, SCHEDULER\n",
        "# =========================================================\n",
        "criterion = nn.CrossEntropyLoss(weight=weights.to(device), label_smoothing=0.1)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"max\", patience=2, factor=0.5\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# 13) TRAINING LOOP (EARLY STOPPING)\n",
        "# =========================================================\n",
        "def train_model(model, train_loader, val_loader, epochs=40):\n",
        "    best_acc = 0.0\n",
        "    patience = 6\n",
        "    es_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # ---- validate ----\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                preds = outputs.argmax(1)\n",
        "\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_acc = correct / total\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.3f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # ---- save best ----\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            es_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_mobilenet.pth\")\n",
        "        else:\n",
        "            es_counter += 1\n",
        "\n",
        "        # ---- early stop ----\n",
        "        if es_counter >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    print(\"\\nBest Val Accuracy =\", best_acc)\n",
        "    return best_acc\n",
        "\n",
        "# =========================================================\n",
        "# 14) RUN TRAINING\n",
        "# =========================================================\n",
        "train_model(model, train_loader, val_loader, epochs=40)\n",
        "\n",
        "# =========================================================\n",
        "# 15) FINAL VAL ACCURACY CHECK\n",
        "# =========================================================\n",
        "def eval_accuracy(loader, model_path=\"best_mobilenet.pth\"):\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            preds = outputs.argmax(1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "val_acc = eval_accuracy(val_loader)\n",
        "print(\"\\nFinal Validation Accuracy =\", val_acc)\n",
        "\n",
        "# =========================================================\n",
        "# 16) OPTIONAL TEST ACCURACY (IF YOU HAVE TEST CSV)\n",
        "# =========================================================\n",
        "if os.path.exists(TEST_CSV):\n",
        "    test_df = pd.read_csv(TEST_CSV)\n",
        "\n",
        "    # keep only test images that exist\n",
        "    exists_test = test_df[\"image\"].apply(lambda x: os.path.exists(os.path.join(TEST_IMG_DIR, x)))\n",
        "    test_df = test_df[exists_test].reset_index(drop=True)\n",
        "\n",
        "    test_df[\"label_idx\"] = test_df[\"label\"].map(label_to_idx)\n",
        "\n",
        "    test_dataset = CarDamageDataset(test_df, TEST_IMG_DIR, val_transform)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    test_acc = eval_accuracy(test_loader)\n",
        "    print(\"Final TEST Accuracy =\", test_acc)\n",
        "else:\n",
        "    print(\"\\nNo TEST CSV found, skipping test evaluation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "860ab160e1a0429590025727b347b4d4",
            "d6add4f08da74723b822408496628e96",
            "112d9ec0477449fabe62566dd7cc1abe",
            "a5a27392165c444f8136973085b604ec",
            "ffd624884c6d4265a9a0958c7e6c80e3",
            "e0f46bf88ac949039c27335919e16b63",
            "ede21b36b3944f4595626197cae101cd",
            "fee37eb7563747918561d0bf34cb9790",
            "ed4d0b255c444ffea310969f1a74e29a",
            "8fb3c7b7faa54d48b8760ec34a2543e6",
            "facbe96ef0df4da79378caabbe1a1fe8"
          ]
        },
        "id": "iNJLtjiFah9F",
        "outputId": "7434c7d9-20db-463f-a637-6f80f08caf68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.11.12)\n",
            "        image          label\n",
            "0  000649.jpg        scratch\n",
            "1  001666.jpg           dent\n",
            "2  001693.jpg  glass shatter\n",
            "3  003449.jpg           dent\n",
            "4  002344.jpg        scratch\n",
            "5  002137.jpg        scratch\n",
            "6  002367.jpg           dent\n",
            "7  003837.jpg        scratch\n",
            "8  003868.jpg          crack\n",
            "9  000963.jpg           dent\n",
            "\n",
            "Label counts:\n",
            " label\n",
            "scratch          1073\n",
            "dent              967\n",
            "glass shatter     391\n",
            "tire flat         174\n",
            "crack             141\n",
            "lamp broken        70\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Missing images: 0\n",
            "\n",
            "Train split counts:\n",
            " label\n",
            "scratch          912\n",
            "dent             822\n",
            "glass shatter    332\n",
            "tire flat        148\n",
            "crack            120\n",
            "lamp broken       59\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Val split counts:\n",
            " label\n",
            "scratch          161\n",
            "dent             145\n",
            "glass shatter     59\n",
            "tire flat         26\n",
            "crack             21\n",
            "lamp broken       11\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Classes: ['crack', 'dent', 'glass shatter', 'lamp broken', 'scratch', 'tire flat']\n",
            "num_classes: 6\n",
            "\n",
            "Train samples: 2393\n",
            "Val samples: 423\n",
            "\n",
            "Class weights: tensor([0.2230, 0.0326, 0.0806, 0.4536, 0.0293, 0.1808])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/14.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "860ab160e1a0429590025727b347b4d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 | Loss: 212.413 | Val Acc: 0.3924\n",
            "Epoch 2/40 | Loss: 140.693 | Val Acc: 0.4232\n",
            "Epoch 3/40 | Loss: 123.269 | Val Acc: 0.4515\n",
            "Epoch 4/40 | Loss: 113.105 | Val Acc: 0.4657\n",
            "Epoch 5/40 | Loss: 105.618 | Val Acc: 0.5083\n",
            "Epoch 6/40 | Loss: 100.641 | Val Acc: 0.5461\n",
            "Epoch 7/40 | Loss: 98.911 | Val Acc: 0.5154\n",
            "Epoch 8/40 | Loss: 94.705 | Val Acc: 0.5579\n",
            "Epoch 9/40 | Loss: 94.335 | Val Acc: 0.5603\n",
            "Epoch 10/40 | Loss: 89.481 | Val Acc: 0.5319\n",
            "Epoch 11/40 | Loss: 89.268 | Val Acc: 0.5603\n",
            "Epoch 12/40 | Loss: 87.554 | Val Acc: 0.5863\n",
            "Epoch 13/40 | Loss: 86.348 | Val Acc: 0.6076\n",
            "Epoch 14/40 | Loss: 83.239 | Val Acc: 0.6052\n",
            "Epoch 15/40 | Loss: 84.190 | Val Acc: 0.6052\n",
            "Epoch 16/40 | Loss: 81.958 | Val Acc: 0.6312\n",
            "Epoch 17/40 | Loss: 82.072 | Val Acc: 0.6241\n",
            "Epoch 18/40 | Loss: 80.601 | Val Acc: 0.6123\n",
            "Epoch 19/40 | Loss: 80.912 | Val Acc: 0.6194\n",
            "Epoch 20/40 | Loss: 78.288 | Val Acc: 0.6028\n",
            "Epoch 21/40 | Loss: 79.224 | Val Acc: 0.6194\n",
            "Epoch 22/40 | Loss: 78.135 | Val Acc: 0.6336\n",
            "Epoch 23/40 | Loss: 77.366 | Val Acc: 0.6336\n",
            "Epoch 24/40 | Loss: 77.916 | Val Acc: 0.6359\n",
            "Epoch 25/40 | Loss: 77.069 | Val Acc: 0.6383\n",
            "Epoch 26/40 | Loss: 76.753 | Val Acc: 0.6359\n",
            "Epoch 27/40 | Loss: 75.549 | Val Acc: 0.6076\n",
            "Epoch 28/40 | Loss: 75.266 | Val Acc: 0.6359\n",
            "Epoch 29/40 | Loss: 74.914 | Val Acc: 0.6407\n",
            "Epoch 30/40 | Loss: 74.398 | Val Acc: 0.6336\n",
            "Epoch 31/40 | Loss: 74.581 | Val Acc: 0.6312\n",
            "Epoch 32/40 | Loss: 75.698 | Val Acc: 0.6359\n",
            "Epoch 33/40 | Loss: 73.959 | Val Acc: 0.6478\n",
            "Epoch 34/40 | Loss: 74.820 | Val Acc: 0.6454\n",
            "Epoch 35/40 | Loss: 74.754 | Val Acc: 0.6217\n",
            "Epoch 36/40 | Loss: 75.323 | Val Acc: 0.6312\n",
            "Epoch 37/40 | Loss: 74.186 | Val Acc: 0.6336\n",
            "Epoch 38/40 | Loss: 73.781 | Val Acc: 0.6288\n",
            "Epoch 39/40 | Loss: 75.535 | Val Acc: 0.6312\n",
            "Early stopping!\n",
            "\n",
            "Best Val Accuracy = 0.6477541371158393\n",
            "\n",
            "Final Validation Accuracy = 0.6477541371158393\n",
            "Final TEST Accuracy = 0.6925133689839572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ikzmHJU0ax_a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}